# app/agents/nodes.py
"""
LangGraph èŠ‚ç‚¹å®ç°
"""
import json
import logging
from typing import Dict, Any
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from app.utils.model_factory import ModelFactory, ModelType
from app.agents.state import AgentState, UserProfile
from app.agents.prompts import (
    INTENT_ROUTER_PROMPT,
    RAG_GENERATION_PROMPT,
    USER_PROFILE_EXTRACTION_PROMPT
)

logger = logging.getLogger(__name__)


# ==========================================
# 1. æ„å›¾è¯†åˆ«èŠ‚ç‚¹ (Intent Router)
# ==========================================
async def intent_router_node(state: AgentState) -> Dict[str, Any]:
    """
    åˆ†æç”¨æˆ·æœ€æ–°æ¶ˆæ¯ï¼Œåˆ¤æ–­è·¯ç”±æ–¹å‘
    
    è¿”å›:
        next_step: "rag" | "enrichment" | "trade" | "chat" | "end"
    """
    from app.core.logging_config import StructuredLogger, log_performance
    import time
    
    start_time = time.time()
    logger_struct = StructuredLogger("agent.intent_router")
    
    messages = state["messages"]
    user_profile = state.get("user_profile", {})
    
    # è·å–æœ€æ–°çš„ç”¨æˆ·æ¶ˆæ¯
    user_message = ""
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            user_message = msg.content
            break
    
    logger_struct.log_event("intent_analysis_start", {
        "user_message": user_message[:100],  # æˆªå–å‰100å­—ç¬¦
        "user_profile": user_profile
    })
    
    # è·å–æœ€è¿‘3è½®å¯¹è¯å†å²
    recent_history = messages[-6:] if len(messages) >= 6 else messages
    history_text = "\n".join([
        f"{'ç”¨æˆ·' if isinstance(m, HumanMessage) else 'AI'}: {m.content}" 
        for m in recent_history
    ])
    
    # ä½¿ç”¨"å¤§è„‘"æ¨¡å‹ï¼ˆqwen2.5-coder:14bï¼‰è¿›è¡Œæ„å›¾è¯†åˆ«
    llm = ModelFactory.get_brain_model(temperature=0.1)
    
    prompt = ChatPromptTemplate.from_template(INTENT_ROUTER_PROMPT)
    chain = prompt | llm | StrOutputParser()
    
    try:
        result = await chain.ainvoke({
            "user_message": user_message,
            "user_profile": json.dumps(user_profile, ensure_ascii=False),
            "recent_history": history_text
        })
        
        # è§£æJSONç»“æœï¼ˆå¤„ç† Markdown ä»£ç å—ï¼‰
        from app.utils.json_extractor import safe_json_loads
        intent_data = safe_json_loads(result, default={"intent": "rag", "confidence": 0.5, "reasoning": "JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è·¯ç”±"})
        next_step = intent_data.get("intent", "rag")
        
        # æ›´æ–°ç”¨æˆ·ç”»åƒï¼ˆå¦‚æœæå–åˆ°æ–°å®ä½“ï¼‰
        extracted_entities = intent_data.get("extracted_entities", {})
        updated_profile = _update_user_profile(user_profile, extracted_entities)
        
        elapsed_ms = int((time.time() - start_time) * 1000)
        
        logger.info(f"[Intent Router] è¯†åˆ«æ„å›¾: {next_step}, ç½®ä¿¡åº¦: {intent_data.get('confidence', 0)}")
        logger.info(f"[Intent Router] æ¨ç†: {intent_data.get('reasoning', '')}")
        
        # è®°å½•ç»“æ„åŒ–æ—¥å¿—
        logger_struct.log_event("intent_analysis_complete", {
            "next_step": next_step,
            "confidence": intent_data.get("confidence", 0),
            "reasoning": intent_data.get("reasoning", ""),
            "extracted_entities": extracted_entities,
            "elapsed_ms": elapsed_ms
        })
        
        return {
            "next_step": next_step,
            "user_profile": updated_profile,
            "metadata": {
                "intent_confidence": intent_data.get("confidence", 0),
                "intent_reasoning": intent_data.get("reasoning", ""),
                "elapsed_ms": elapsed_ms
            }
        }
        
    except Exception as e:
        elapsed_ms = int((time.time() - start_time) * 1000)
        
        logger.error(f"[Intent Router] è§£æå¤±è´¥ï¼Œé»˜è®¤è·¯ç”±åˆ°RAG: {e}")
        
        logger_struct.log_event("intent_analysis_failed", {
            "error": str(e),
            "fallback_step": "rag",
            "elapsed_ms": elapsed_ms
        }, level="ERROR")
        
        return {"next_step": "rag"}


# ==========================================
# 2. RAG èŠ‚ç‚¹ (Retrieval-Augmented Generation)
# ==========================================
async def rag_node(state: AgentState) -> Dict[str, Any]:
    """
    è°ƒç”¨æ··åˆæ£€ç´¢ï¼ˆES + Milvusï¼‰ï¼Œç”ŸæˆåŸºäºContextçš„å›ç­”
    """
    from app.core.logging_config import StructuredLogger
    import time
    
    start_time = time.time()
    logger_struct = StructuredLogger("agent.rag")
    
    messages = state["messages"]
    user_profile = state.get("user_profile", {})
    
    # è·å–ç”¨æˆ·é—®é¢˜
    user_question = ""
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            user_question = msg.content
            break
    
    # ==========================================
    # æ¥å…¥æ··åˆæ£€ç´¢æœåŠ¡ï¼ˆES + Milvusï¼‰
    # ==========================================
    try:
        from app.services.hybrid_search_service import hybrid_search
        
        # æ‰§è¡Œæ··åˆæ£€ç´¢
        search_results = await hybrid_search(user_question, top_k=5)
        
        if search_results:
            # æ„é€  Context
            context_parts = []
            for idx, result in enumerate(search_results, 1):
                context_parts.append(f"""
ã€è½¦å‹ {idx}ã€‘{result.name}
- å“ç‰Œ: {result.brand_name}
- è½¦ç³»: {result.series_name}
- ä»·æ ¼: {result.price}ä¸‡
- èƒ½æºç±»å‹: {result.energy_type}
- çº§åˆ«: {result.series_level}
- æ ‡ç­¾: {result.tags_text or 'æš‚æ— '}
- ç›¸å…³åº¦å¾—åˆ†: {result.score:.2f}
""")
            context = "\n".join(context_parts)
            
            logger.info(f"[RAG Node] æ··åˆæ£€ç´¢æ‰¾åˆ° {len(search_results)} æ¡ç»“æœ")
            
            # è®°å½•æ£€ç´¢æ—¥å¿—
            logger_struct.log_search(
                search_type="hybrid",
                query=user_question[:100],
                results_count=len(search_results),
                elapsed_ms=int((time.time() - start_time) * 1000)
            )
        else:
            # æ£€ç´¢ä¸ºç©ºï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            logger.warning(f"[RAG Node] æ··åˆæ£€ç´¢æ— ç»“æœï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            context = _mock_rag_retrieve(user_question, user_profile)
    
    except Exception as e:
        logger.error(f"[RAG Node] æ··åˆæ£€ç´¢å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®: {e}")
        context = _mock_rag_retrieve(user_question, user_profile)
    
    # å¦‚æœæ£€ç´¢ç»“æœä¸ºç©ºï¼Œè·¯ç”±åˆ°æ•°æ®è¡¥å……èŠ‚ç‚¹
    if not context or len(context) < 50:
        print("[RAG Node] æ£€ç´¢ç»“æœä¸ºç©ºï¼Œè·¯ç”±åˆ°æ•°æ®è¡¥å……èŠ‚ç‚¹")
        return {
            "rag_context": context,
            "next_step": "enrichment"
        }
    
    # ä½¿ç”¨"å¿«å˜´"æ¨¡å‹ï¼ˆqwen2.5:7bï¼‰ç”Ÿæˆå›ç­”ï¼ˆåŸºäºæ£€ç´¢ç»“æœï¼Œä»»åŠ¡ç®€å•ï¼‰
    llm = ModelFactory.get_quick_model(temperature=0.7)
    prompt = ChatPromptTemplate.from_template(RAG_GENERATION_PROMPT)
    chain = prompt | llm | StrOutputParser()
    
    answer = await chain.ainvoke({
        "question": user_question,
        "context": context,
        "budget": f"{user_profile.get('budget_min', 'æœªçŸ¥')}-{user_profile.get('budget_max', 'æœªçŸ¥')}ä¸‡",
        "city": user_profile.get("city", "æœªçŸ¥"),
        "preferences": json.dumps(user_profile.get("preferences", {}), ensure_ascii=False)
    })
    
    elapsed_ms = int((time.time() - start_time) * 1000)
    
    logger.info(f"[RAG Node] ç”Ÿæˆå›ç­”é•¿åº¦: {len(answer)} å­—ç¬¦")
    
    # è®°å½• RAG å®Œæˆæ—¥å¿—
    logger_struct.log_event("rag_complete", {
        "answer_length": len(answer),
        "context_length": len(context),
        "elapsed_ms": elapsed_ms
    })
    
    return {
        "rag_context": context,
        "final_answer": answer,
        "next_step": "end",
        "metadata": {"elapsed_ms": elapsed_ms}
    }


# ==========================================
# 3. æ•°æ®è¡¥å……èŠ‚ç‚¹ (Data Enrichment)
# ==========================================
async def data_enrichment_node(state: AgentState) -> Dict[str, Any]:
    """
    è°ƒç”¨æ•°æ®è¡¥å……å­Agentï¼Œè·å–å®æ—¶æ•°æ®
    """
    messages = state["messages"]
    user_profile = state.get("user_profile", {})
    
    # è·å–ç”¨æˆ·é—®é¢˜
    user_question = ""
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            user_question = msg.content
            break
    
    # ==========================================
    # è°ƒç”¨æ•°æ®è¡¥å…… Graph Agent
    # ==========================================
    try:
        from app.agents.enrichment_graph import DataEnrichmentAgent
        
        # æå–è½¦ç³»åç§°ï¼ˆç®€å•ç¤ºä¾‹ï¼‰
        car_series_name = _extract_car_series_from_question(user_question)
        
        if not car_series_name:
            # å¦‚æœæ— æ³•æå–è½¦ç³»åç§°ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            enrichment_result = _mock_data_enrichment(user_question, user_profile)
        else:
            # è°ƒç”¨æ•°æ®è¡¥å…… Agent
            enrichment_agent = DataEnrichmentAgent()
            result = await enrichment_agent.enrich(
                car_series_name=car_series_name,
                force_refresh=False,
                user_city=user_profile.get("city")
            )
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            enrichment_result = {
                "success": result.success,
                "data": result.data or {},
                "message": result.message,
                "update_time": result.update_time
            }
            
            print(f"[DataEnrichment] æ•°æ®è¡¥å……å®Œæˆ: {result.source}")
    
    except Exception as e:
        print(f"[DataEnrichment] è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®: {e}")
        enrichment_result = _mock_data_enrichment(user_question, user_profile)
    
    # ä½¿ç”¨"å¿«å˜´"æ¨¡å‹ç”Ÿæˆå›ç­”
    llm = ModelFactory.get_quick_model(temperature=0.7)
    
    enriched_context = f"""
æˆ‘åˆšåˆšä¸ºæ‚¨æ›´æ–°äº†æœ€æ–°æ•°æ®ï¼ˆæ›´æ–°æ—¶é—´: {enrichment_result.get('update_time', 'æœªçŸ¥')}ï¼‰ï¼š

{json.dumps(enrichment_result.get('data', {}), ensure_ascii=False, indent=2)}
"""
    
    prompt = ChatPromptTemplate.from_template(RAG_GENERATION_PROMPT)
    chain = prompt | llm | StrOutputParser()
    
    answer = await chain.ainvoke({
        "question": user_question,
        "context": enriched_context,
        "budget": f"{user_profile.get('budget_min', 'æœªçŸ¥')}-{user_profile.get('budget_max', 'æœªçŸ¥')}ä¸‡",
        "city": user_profile.get("city", "æœªçŸ¥"),
        "preferences": json.dumps(user_profile.get("preferences", {}), ensure_ascii=False)
    })
    
    # åœ¨ç­”æ¡ˆå‰åŠ ä¸Šæ˜ç¡®çš„æ•°æ®æ›´æ–°æç¤º
    final_answer = f"âœ… æˆ‘åˆšåˆšä¸ºæ‚¨æ›´æ–°äº†è¿™æ¬¾è½¦çš„æœ€æ–°æ•°æ®...\n\n{answer}"
    
    print(f"[Data Enrichment] æ•°æ®è¡¥å……å®Œæˆ")
    
    return {
        "enrichment_result": enrichment_result,
        "final_answer": final_answer,
        "next_step": "end"
    }


# ==========================================
# 4. äº¤æ˜“èŠ‚ç‚¹ (Trade)
# ==========================================
async def trade_node(state: AgentState) -> Dict[str, Any]:
    """
    å¤„ç†äº¤æ˜“ç›¸å…³è¯·æ±‚ï¼ˆè®¢é‡‘ã€è¯•é©¾ã€äºŒæ‰‹è½¦ä¼°ä»·ï¼‰
    """
    messages = state["messages"]
    user_profile = state.get("user_profile", {})
    
    # è·å–ç”¨æˆ·é—®é¢˜
    user_question = ""
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            user_question = msg.content
            break
    
    # è°ƒç”¨äº¤æ˜“æœåŠ¡
    try:
        from app.services.trade_service import TradeService
        trade_info = await TradeService.handle_request(
            query=user_question,
            user_id=state.get("metadata", {}).get("user_id"),  # ä» state metadata ä¸­è·å–
            user_city=user_profile.get("city")
        )
    except Exception as e:
        logger.error(f"[Trade Node] äº¤æ˜“æœåŠ¡è°ƒç”¨å¤±è´¥: {e}")
        trade_info = _mock_trade_service(user_question, user_profile)
    
    # ä½¿ç”¨"å¿«å˜´"æ¨¡å‹ç”Ÿæˆå›ç­”
    llm = ModelFactory.get_quick_model(temperature=0.7)
    
    trade_prompt = f"""
ä½ æ˜¯æ˜“è½¦è´­è½¦ç®¡å®¶ï¼Œç”¨æˆ·å’¨è¯¢äº¤æ˜“ç›¸å…³é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜: {user_question}

äº¤æ˜“ä¿¡æ¯:
{json.dumps(trade_info, ensure_ascii=False, indent=2)}

è¯·ç”Ÿæˆå‹å¥½çš„å›å¤ï¼Œå¦‚æœæ¶‰åŠé‡‘é’±äº¤æ˜“ï¼ŒåŠ¡å¿…æé†’ç”¨æˆ·ï¼š
"æ¸©é¦¨æç¤ºï¼šæ”¯ä»˜å‰è¯·ä»”ç»†é˜…è¯»åè®®æ¡æ¬¾ï¼Œå¦‚æœ‰ç–‘é—®å¯è”ç³»å®¢æœã€‚"

å›å¤:
"""
    
    answer = await llm.ainvoke(trade_prompt)
    final_answer = answer.content if hasattr(answer, 'content') else str(answer)
    
    print(f"[Trade Node] äº¤æ˜“è¯·æ±‚å¤„ç†å®Œæˆ")
    
    return {
        "trade_info": trade_info,
        "final_answer": final_answer,
        "next_step": "end"
    }


# ==========================================
# 5. é—²èŠèŠ‚ç‚¹ (Chat)
# ==========================================
async def chat_node(state: AgentState) -> Dict[str, Any]:
    """
    å¤„ç†é—²èŠã€æ‰“æ‹›å‘¼ç­‰éä¸šåŠ¡å¯¹è¯
    """
    messages = state["messages"]
    
    # è·å–ç”¨æˆ·æ¶ˆæ¯
    user_message = ""
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            user_message = msg.content
            break
    
    # ç®€å•çš„è§„åˆ™åŒ¹é…
    greetings = ["ä½ å¥½", "æ‚¨å¥½", "hi", "hello", "æ—©ä¸Šå¥½", "æ™šä¸Šå¥½"]
    thanks = ["è°¢è°¢", "æ„Ÿè°¢", "å¤šè°¢", "thanks", "thank you"]
    
    if any(g in user_message.lower() for g in greetings):
        answer = "æ‚¨å¥½ï¼æˆ‘æ˜¯æ˜“è½¦æ™ºèƒ½è´­è½¦ç®¡å®¶ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚æ‚¨å¯ä»¥é—®æˆ‘å…³äºæ±½è½¦çš„ä»»ä½•é—®é¢˜ï¼Œæ¯”å¦‚æ¨èè½¦å‹ã€æŸ¥è¯¢ä»·æ ¼ã€å¯¹æ¯”å‚æ•°ç­‰ã€‚è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„å—ï¼ŸğŸ˜Š"
    elif any(t in user_message.lower() for t in thanks):
        answer = "ä¸å®¢æ°”ï¼å¦‚æœè¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œéšæ—¶é—®æˆ‘å“¦ã€‚ç¥æ‚¨æ—©æ—¥æ‰¾åˆ°å¿ƒä»ªçš„çˆ±è½¦ï¼ğŸš—"
    else:
        # ä½¿ç”¨"å¿«å˜´"æ¨¡å‹ç”Ÿæˆé—²èŠå›å¤ï¼ˆé€Ÿåº¦å¿«ï¼‰
        llm = ModelFactory.get_quick_model(temperature=0.9)
        chat_prompt = f"""
ä½ æ˜¯æ˜“è½¦è´­è½¦ç®¡å®¶ï¼Œç”¨æˆ·å‘æ¥é—²èŠæ¶ˆæ¯: "{user_message}"

è¯·ç”¨å‹å¥½ã€è‡ªç„¶çš„æ–¹å¼å›å¤ï¼Œé€‚å½“å¼•å¯¼ç”¨æˆ·æå‡ºè´­è½¦ç›¸å…³é—®é¢˜ã€‚
å›å¤è¦ç®€çŸ­ï¼ˆä¸è¶…è¿‡50å­—ï¼‰ï¼Œè¯­æ°”è½»æ¾ã€‚

å›å¤:
"""
        answer_msg = await llm.ainvoke(chat_prompt)
        answer = answer_msg.content if hasattr(answer_msg, 'content') else str(answer_msg)
    
    print(f"[Chat Node] é—²èŠå›å¤")
    
    return {
        "final_answer": answer,
        "next_step": "end"
    }


# ==========================================
# è¾…åŠ©å‡½æ•°
# ==========================================
def _update_user_profile(current_profile: UserProfile, extracted_entities: Dict[str, Any]) -> UserProfile:
    """æ›´æ–°ç”¨æˆ·ç”»åƒ"""
    updated = current_profile.copy()
    
    # æ›´æ–°é¢„ç®—
    if "budget" in extracted_entities and isinstance(extracted_entities["budget"], list):
        updated["budget_min"] = extracted_entities["budget"][0]
        updated["budget_max"] = extracted_entities["budget"][1]
    
    # æ›´æ–°åŸå¸‚
    if "city" in extracted_entities:
        updated["city"] = extracted_entities["city"]
    
    # æ›´æ–°åå¥½
    preferences = updated.get("preferences", {})
    for key in ["car_brand", "car_series", "energy_type", "level"]:
        if key in extracted_entities:
            preferences[key] = extracted_entities[key]
    updated["preferences"] = preferences
    
    return updated


def _mock_rag_retrieve(question: str, user_profile: UserProfile) -> str:
    """æ¨¡æ‹ŸRAGæ£€ç´¢ï¼ˆå¼€å‘é˜¶æ®µï¼‰"""
    # ç®€å•çš„å…³é”®è¯åŒ¹é…
    if "ç§¦PLUS" in question or "ç§¦plus" in question.lower():
        return """
ã€æ¯”äºšè¿ªç§¦PLUS DM-iã€‘
- 2026æ¬¾ 120km å† å†›ç‰ˆ: æŒ‡å¯¼ä»· 11.98ä¸‡ï¼Œå®é™…ä¼˜æƒ åçº¦ 10.98ä¸‡
- 2026æ¬¾ 120km å°Šè´µå‹: æŒ‡å¯¼ä»· 13.98ä¸‡
- èƒ½æºç±»å‹: æ’ç”µå¼æ··åˆåŠ¨åŠ›
- NEDCçº¯ç”µç»­èˆª: 120km
- ç»¼åˆæ²¹è€—: 3.8L/100km
- è½¦èº«çº§åˆ«: ç´§å‡‘å‹è½¦
- åº§ä½æ•°: 5åº§
- è¡¥è´´æ”¿ç­–: éƒ¨åˆ†åŸå¸‚å¯äº«å—æ–°èƒ½æºè¡¥è´´å’Œå…è´­ç½®ç¨
"""
    elif "SUV" in question or "suv" in question.lower():
        return """
ã€20ä¸‡å·¦å³SUVæ¨èã€‘
1. æ¯”äºšè¿ªå®‹PLUS DM-i: 15.98-21.98ä¸‡ï¼Œæ’ç”µæ··åŠ¨ï¼Œç»­èˆª110km
2. ç†æƒ³L6: 24.98ä¸‡èµ·ï¼Œå¢ç¨‹å¼ï¼Œç»­èˆª210km
3. æœ¬ç”°CR-V: 18.59-26.39ä¸‡ï¼Œç‡ƒæ²¹/æ··åŠ¨å¯é€‰
4. å¤§ä¼—é€”è§‚L: 21.58-28.58ä¸‡ï¼Œç‡ƒæ²¹ï¼Œç©ºé—´å¤§
"""
    else:
        return ""  # è§¦å‘enrichment


def _mock_data_enrichment(question: str, user_profile: UserProfile) -> Dict[str, Any]:
    """æ¨¡æ‹Ÿæ•°æ®è¡¥å……æœåŠ¡"""
    return {
        "success": True,
        "data": {
            "car_series": "æ¯”äºšè¿ªç§¦PLUS",
            "models": [
                {
                    "name": "2026æ¬¾ DM-i 120km å† å†›ç‰ˆ",
                    "price_guidance": 11.98,
                    "price_real": 10.98,
                    "subsidy": 1.5,
                    "dealer_discount": 0.5
                }
            ],
            "local_policy": f"{user_profile.get('city', 'æœ¬åœ°')}åœ°åŒºå¯äº«å—æ–°èƒ½æºè¡¥è´´1.5ä¸‡å…ƒ"
        },
        "update_time": "2026-01-15 10:30:00"
    }


def _mock_trade_service(question: str, user_profile: UserProfile) -> Dict[str, Any]:
    """æ¨¡æ‹Ÿäº¤æ˜“æœåŠ¡"""
    if "è¯•é©¾" in question:
        return {
            "type": "test_drive",
            "message": "å·²ä¸ºæ‚¨æ¨èé™„è¿‘çš„ç»é”€å•†",
            "dealers": [
                {"name": "XXæ¯”äºšè¿ª4Såº—", "phone": "010-12345678", "address": "åŒ—äº¬å¸‚æœé˜³åŒºXXè·¯XXå·"}
            ]
        }
    elif "ä¼°ä»·" in question or "ç½®æ¢" in question:
        return {
            "type": "trade_in",
            "message": "äºŒæ‰‹è½¦ä¼°ä»·éœ€è¦æä¾›è½¦è¾†è¯¦ç»†ä¿¡æ¯",
            "required_info": ["å“ç‰Œ", "è½¦ç³»", "å¹´æ¬¾", "é‡Œç¨‹", "è½¦å†µ"]
        }
    else:
        return {
            "type": "order",
            "message": "è®¢é‡‘æ”¯ä»˜åŠŸèƒ½å¼€å‘ä¸­ï¼Œæ•¬è¯·æœŸå¾…"
        }


def _extract_car_series_from_question(question: str) -> str:
    """
    ä»ç”¨æˆ·é—®é¢˜ä¸­æå–è½¦ç³»åç§°
    
    ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ + å…³é”®è¯åº“åŒ¹é…
    """
    import re
    
    # å·²çŸ¥è½¦ç³»åº“ï¼ˆæŒ‰é•¿åº¦æ’åºï¼Œä¼˜å…ˆåŒ¹é…é•¿çš„ï¼‰
    known_series = [
        "ç§¦PLUS", "ç§¦Plus", "ç§¦plus", "ç§¦",
        "æ±‰", "å”", "å®‹PLUS", "æµ·è±¹", "æµ·é¸¥", "å…ƒPLUS",
        "Model 3", "Model Y", "Model S", "Model X",
        "ç†æƒ³L6", "ç†æƒ³L7", "ç†æƒ³L8", "ç†æƒ³L9", "ç†æƒ³ONE",
        "å°é¹P7", "å°é¹G9", "å°é¹G6", "å°é¹P5",
        "è”šæ¥ET5", "è”šæ¥ES6", "è”šæ¥ET7", "è”šæ¥ES8",
        "å¥¥è¿ªA4L", "å¥¥è¿ªA6L", "å¥¥è¿ªQ5L", "å¥¥è¿ªQ7",
        "å®é©¬3ç³»", "å®é©¬5ç³»", "å®é©¬X3", "å®é©¬X5",
        "å¥”é©°Cçº§", "å¥”é©°Eçº§", "å¥”é©°GLC", "å¥”é©°GLE",
        "æœ¬ç”°CR-V", "æœ¬ç”°é›…é˜", "æœ¬ç”°æ€åŸŸ", "æœ¬ç”°é£åº¦",
        "å¤§ä¼—é€”è§‚L", "å¤§ä¼—å¸•è¨ç‰¹", "å¤§ä¼—é«˜å°”å¤«", "å¤§ä¼—æœ—é€¸",
        "ä¸°ç”°å¡ç½—æ‹‰", "ä¸°ç”°å‡¯ç¾ç‘", "ä¸°ç”°RAV4", "ä¸°ç”°æ±‰å…°è¾¾"
    ]
    known_series.sort(key=len, reverse=True)
    
    question_lower = question.lower()
    
    # ç²¾ç¡®åŒ¹é…
    for series in known_series:
        if series.lower() in question_lower:
            return series
    
    # æ­£åˆ™æ¨¡å¼åŒ¹é…ï¼ˆå¦‚ "L6"ã€"ES6" ç­‰ï¼‰
    patterns = [
        r'[A-Z]+\d+[A-Z]*',  # åŒ¹é… A4L, ES6, L7 ç­‰
        r'[\u4e00-\u9fff]+(?:PLUS|Plus|plus)?',  # åŒ¹é…ä¸­æ–‡+PLUS
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, question)
        if matches:
            # è¿”å›æœ€é•¿çš„åŒ¹é…
            return max(matches, key=len)
    
    return ""
