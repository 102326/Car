import httpx
import asyncio
import random
from typing import List, Dict
from bs4 import BeautifulSoup


class ArticleData:
    def __init__(self, title: str, url: str, source: str, cover: str = "", publish_time: str = ""):
        self.title = title
        self.url = url
        self.source = source
        self.cover = cover
        self.publish_time = publish_time

    def to_dict(self):
        return {
            "title": self.title,
            "url": self.url,
            "source": self.source,
            "cover": self.cover,
            "publish_time": self.publish_time
        }


class AutoNewsCrawler:
    """
    æ±½è½¦èµ„è®¯èšåˆçˆ¬è™«æœåŠ¡ (ç¨³å®šç‰ˆ)
    """

    def __init__(self):
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/123.0"
        ]

    def _get_headers(self):
        return {
            "User-Agent": random.choice(self.user_agents),
            "Referer": "https://www.autohome.com.cn/",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        }

    # 1. æ±½è½¦ä¹‹å®¶ (HTMLæ¨¡å¼)
    async def fetch_autohome_channel(self, client, url, channel_name) -> List[ArticleData]:
        articles = []
        try:
            # éšæœºå»¶æ—¶ (é‡è¦ï¼šé˜²å°)
            await asyncio.sleep(random.uniform(1.5, 3.5))

            resp = await client.get(url, timeout=10.0)
            if resp.status_code != 200:
                print(f"âš ï¸ [æ±½è½¦ä¹‹å®¶-{channel_name}] è¯·æ±‚å¤±è´¥: {resp.status_code}")
                return []

            content = resp.content.decode("gbk", errors="ignore")
            soup = BeautifulSoup(content, "html.parser")

            # åŒ¹é…å¤šç§åˆ—è¡¨ç»“æ„
            news_list = soup.select("#auto-channel-lazyload-article li, .article-wrapper li, .tab-content-item li")

            for item in news_list:
                if not item.find("h3"): continue

                title_tag = item.select_one("h3")
                link_tag = item.select_one("a")
                img_tag = item.select_one("img")

                if title_tag and link_tag:
                    img_url = img_tag.get("src") or img_tag.get("data-original") if img_tag else ""
                    if img_url and img_url.startswith("//"): img_url = "https:" + img_url

                    link = link_tag.get("href")
                    if link and link.startswith("//"): link = "https:" + link

                    if "autohome.com.cn/news/" in link or "autohome.com.cn/advice/" in link or "autohome.com.cn/drive/" in link:
                        articles.append(ArticleData(
                            title=title_tag.get_text(strip=True),
                            url=link,
                            source=f"æ±½è½¦ä¹‹å®¶-{channel_name}",
                            cover=img_url
                        ))
        except Exception as e:
            # ä»…æ‰“å°ç®€ç•¥é”™è¯¯ï¼Œé¿å…åˆ·å±
            pass

        return articles

    async def fetch_autohome_deep(self) -> List[ArticleData]:
        """
        ä¿®å¤ç‰ˆï¼šé’ˆå¯¹ 404 é—®é¢˜è¿›è¡Œè°ƒæ•´ã€‚
        ä»…æŠ“å– 'æœ€æ–°' é¢‘é“çš„å‰ 20 é¡µï¼Œå› ä¸ºå…¶ä»–é¢‘é“çš„åˆ†é¡µè§„åˆ™å¯èƒ½å·²å˜æ›´ã€‚
        """
        # 1. åŸºç¡€é¢‘é“ (åªæŠ“é¦–é¡µ)
        base_channels = [
            ("æœ€æ–°", "https://www.autohome.com.cn/all/"),
            ("æ–°é—»", "https://www.autohome.com.cn/news/"),
            ("è¯„æµ‹", "https://www.autohome.com.cn/drive/"),
            ("å¯¼è´­", "https://www.autohome.com.cn/advice/"),
        ]
        
        target_urls = []
        for name, url in base_channels:
            target_urls.append((name, url))

        # 2. å°è¯•æŠ“å– "æœ€æ–°" é¢‘é“çš„ç¬¬ 2-20 é¡µ
        # ç»è¿‡éªŒè¯ï¼Œ"å…¨éƒ¨"é¢‘é“çš„è§„åˆ™é€šå¸¸æ˜¯: https://www.autohome.com.cn/all/2/
        # æ³¨æ„ï¼šæœ«å°¾çš„æ–œæ å¾ˆé‡è¦
        for page in range(2, 21):
            # è¿™ç§æ ¼å¼é€šå¸¸æ›´ç¨³å®š: /all/é¡µç /
            url = f"https://www.autohome.com.cn/all/{page}/"
            target_urls.append((f"æœ€æ–°-P{page}", url))

        print(f"ğŸš€ [æ±½è½¦ä¹‹å®¶] ä¿®å¤æŠ“å–: {len(target_urls)} ä¸ªé¡µé¢")
        
        all_items = []
        async with httpx.AsyncClient(headers=self._get_headers(), follow_redirects=True) as client:
            # é™åˆ¶å¹¶å‘ä¸º 3
            sem = asyncio.Semaphore(3) 
            
            async def limited_fetch(t_url, t_name):
                async with sem:
                    return await self.fetch_autohome_channel(client, t_url, t_name)

            tasks = [limited_fetch(url, name) for name, url in target_urls]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for res in results:
                if isinstance(res, list):
                    all_items.extend(res)
                
        print(f"âœ… [æ±½è½¦ä¹‹å®¶] æŠ“å–å®Œæˆï¼Œå…±è·å– {len(all_items)} æ¡æ•°æ®")
        return all_items

    # 2. æ˜“è½¦ç½‘ (æš‚ç•¥)
    async def fetch_yiche_deep(self) -> List[ArticleData]:
        return []

    # æ€»å…¥å£
    async def run_all(self) -> Dict[str, List[Dict]]:
        results = await asyncio.gather(
            self.fetch_autohome_deep(),
            self.fetch_yiche_deep()
        )
        
        autohome = results[0]
        yiche = results[1]
        all_flat = autohome + yiche

        return {
            "autohome": [a.to_dict() for a in autohome],
            "yiche": [a.to_dict() for a in yiche],
            "all_flat": [a.to_dict() for a in all_flat]
        }

if __name__ == "__main__":
    crawler = AutoNewsCrawler()
    res = asyncio.run(crawler.run_all())
    print(f"æŠ“å–å®Œæˆ: æ€»è®¡ {len(res['all_flat'])} æ¡")
