import asyncio
import json
import logging
import time
from typing import Set, Tuple, List, Dict

from aiokafka import AIOKafkaConsumer, TopicPartition
from sqlalchemy import select

from app.core.database import AsyncSessionLocal
from app.models.car import CarModel, CarSeries
from app.services.car_assembler import fetch_and_assemble_car_docs
from app.services.es_service import CarESService

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("CDC_Sync")


# ==========================================
# ğŸ›¡ï¸ æ™ºèƒ½ç¼“å†²åŒº (Pro Max)
# ==========================================
class SmartBuffer:
    def __init__(self, consumer: AIOKafkaConsumer, max_events=2000, hard_limit=5000, max_wait=1.0):
        self.consumer = consumer
        self.max_events = max_events  # è§¦å‘ Flush çš„è½¯é˜ˆå€¼
        self.hard_limit = hard_limit  # è§¦å‘ Pause çš„ç¡¬é˜ˆå€¼
        self.max_wait = max_wait

        self.event_buffer: Set[Tuple[str, int]] = set()
        self.retry_buffer: Dict[int, int] = {}
        self.MAX_RETRIES = 3

        self.last_flush_time = time.time()
        self._lock = asyncio.Lock()
        self.paused = False

    async def add_event(self, table: str, row_id: int):
        async with self._lock:
            self.event_buffer.add((table, row_id))
            current_size = len(self.event_buffer) + len(self.retry_buffer)

        # 1. ğŸ›‘ æµæ§ (Backpressure)
        if current_size >= self.hard_limit and not self.paused:
            logger.warning(f"ğŸ›‘ Buffer çˆ†æ»¡ ({current_size})ï¼Œæš‚åœæ¶ˆè´¹ Kafka...")
            self.consumer.pause(*self.consumer.assignment())
            self.paused = True

        # 2. è§¦å‘ Flush
        if current_size >= self.max_events:
            await self.flush()

    async def add_retry_ids(self, ids: List[int]):
        """å¤„ç†å¤±è´¥é‡è¯• & æ­»ä¿¡é˜Ÿåˆ— (DLQ)"""
        async with self._lock:
            for i in ids:
                count = self.retry_buffer.get(i, 0) + 1
                if count <= self.MAX_RETRIES:
                    self.retry_buffer[i] = count
                else:
                    # ğŸ’€ æ­»ä¿¡å¤„ç† (DLQ)
                    logger.error(f"ğŸ’€ [DLQ] ID {i} é‡è¯• {count} æ¬¡ä»å¤±è´¥ï¼Œå½»åº•ä¸¢å¼ƒï¼")
                    # âœ… [ä¿®å¤] æ˜¾å¼ä»é‡è¯•é˜Ÿåˆ—ä¸­ç§»é™¤ï¼Œé˜²æ­¢æ­»å¾ªç¯
                    if i in self.retry_buffer:
                        del self.retry_buffer[i]

    async def flush(self):
        """æ ¸å¿ƒåŒæ­¥é€»è¾‘"""
        async with self._lock:
            if not self.event_buffer and not self.retry_buffer:
                return

            # æå–å¿«ç…§
            events = list(self.event_buffer)
            # æš‚å­˜æ—§çš„ retry countsï¼Œç”¨äºåç»­æ¢å¤
            old_retry_counts = self.retry_buffer.copy()

            self.event_buffer.clear()
            self.retry_buffer.clear()

            self.last_flush_time = time.time()

            # â–¶ï¸ æ¢å¤æ¶ˆè´¹
            if self.paused:
                logger.info("â–¶ï¸ Buffer å‹åŠ›é‡Šæ”¾ï¼Œæ¢å¤æ¶ˆè´¹ Kafka...")
                self.consumer.resume(*self.consumer.assignment())
                self.paused = False

        # 1. è§£ææ‰€æœ‰å—å½±å“çš„ Car ID
        impacted_ids = set(old_retry_counts.keys())
        if events:
            resolved = await self._batch_resolve_events(events)
            impacted_ids.update(resolved)

        if not impacted_ids:
            await self._commit_offset()
            return

        logger.info(f"âš¡ [Flush] å¤„ç† {len(impacted_ids)} ä¸ª Car ID")

        # 2. æ‰¹é‡å¤„ç†
        all_ids = list(impacted_ids)
        chunk_size = 500

        for i in range(0, len(all_ids), chunk_size):
            batch_ids = all_ids[i: i + chunk_size]

            found_docs = await fetch_and_assemble_car_docs(batch_ids)
            found_ids = {d['id'] for d in found_docs}
            missing_ids = [bid for bid in batch_ids if bid not in found_ids]

            failed_upsert = []
            if found_docs:
                failed_upsert = await CarESService.bulk_sync_cars(found_docs)

            failed_delete = []
            if missing_ids:
                failed_delete = await CarESService.bulk_delete_cars(missing_ids)

            # æ¢å¤é‡è¯•è®¡æ•°
            current_failed = set(failed_upsert + failed_delete)
            if current_failed:
                async with self._lock:
                    for fid in current_failed:
                        # æ¢å¤ä¹‹å‰çš„è®¡æ•°
                        if fid in old_retry_counts:
                            self.retry_buffer[fid] = old_retry_counts[fid]

                # å†æ¬¡åŠ å…¥é‡è¯•é€»è¾‘ï¼ˆè¿™é‡Œä¼š +1 å¹¶åˆ¤æ–­æ˜¯å¦ DLQï¼‰
                await self.add_retry_ids(list(current_failed))

        await self._commit_offset()

    async def _batch_resolve_events(self, events: List[Tuple[str, int]]) -> Set[int]:
        final_ids = set()
        model_ids = [eid for t, eid in events if t == 'car_model']
        series_ids = [eid for t, eid in events if t == 'car_series']
        brand_ids = [eid for t, eid in events if t == 'car_brand']

        final_ids.update(model_ids)

        async with AsyncSessionLocal() as session:
            chunk_size = 500
            for i in range(0, len(series_ids), chunk_size):
                chunk = series_ids[i:i + chunk_size]
                if not chunk: continue
                stmt = select(CarModel.id).where(CarModel.series_id.in_(chunk))
                res = await session.execute(stmt)
                final_ids.update(res.scalars().all())

            for i in range(0, len(brand_ids), chunk_size):
                chunk = brand_ids[i:i + chunk_size]
                if not chunk: continue
                stmt = (
                    select(CarModel.id)
                    .join(CarSeries, CarModel.series_id == CarSeries.id)
                    .where(CarSeries.brand_id.in_(chunk))
                )
                res = await session.execute(stmt)
                final_ids.update(res.scalars().all())

        return final_ids

    async def _commit_offset(self):
        try:
            await self.consumer.commit()
        except Exception as e:
            logger.error(f"âŒ Offset Commit Failed: {e}")

    async def auto_flush_loop(self):
        while True:
            await asyncio.sleep(0.5)
            async with self._lock:
                should_flush = (
                        (self.event_buffer or self.retry_buffer) and
                        (time.time() - self.last_flush_time > self.max_wait)
                )
            if should_flush:
                await self.flush()


async def consume():
    await CarESService.create_index_if_not_exists()

    consumer = AIOKafkaConsumer(
        'cdc.car.car_model',
        'cdc.car.car_series',
        'cdc.car.car_brand',
        bootstrap_servers='localhost:9092',
        group_id='es_sync_group_max',
        enable_auto_commit=False,
        auto_offset_reset='latest'
    )

    await consumer.start()
    logger.info("ğŸš€ [Pro Max] CDC Consumer Started")

    buffer = SmartBuffer(consumer, max_events=2000, hard_limit=5000, max_wait=1.0)
    asyncio.create_task(buffer.auto_flush_loop())

    try:
        async for msg in consumer:
            if not msg.value: continue
            try:
                data = json.loads(msg.value)
                payload = data.get('payload')
                if not payload: continue

                op = payload['op']
                row = payload.get('before') if op == 'd' else payload.get('after')
                if not row: continue

                table = msg.topic.split('.')[-1]
                row_id = row['id']
                await buffer.add_event(table, row_id)

            except Exception as e:
                logger.error(f"âŒ Parse Error: {e}")
    finally:
        await consumer.stop()


if __name__ == "__main__":
    try:
        asyncio.run(consume())
    except KeyboardInterrupt:
        pass