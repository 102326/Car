import asyncio
import json
import logging
import time
from typing import Set, Tuple, List, Dict

from aiokafka import AIOKafkaConsumer, TopicPartition
from sqlalchemy import select

from app.core.database import AsyncSessionLocal
from app.models.car import CarModel, CarSeries
from app.services.car_assembler import fetch_and_assemble_car_docs
from app.services.es_service import CarESService

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("CDC_Sync")


# ==========================================
# ğŸ›¡ï¸ æ™ºèƒ½ç¼“å†²åŒº (Pro Max)
# ==========================================
class SmartBuffer:
    def __init__(self, consumer: AIOKafkaConsumer, max_events=2000, hard_limit=5000, max_wait=1.0):
        self.consumer = consumer
        self.max_events = max_events  # è§¦å‘ Flush çš„è½¯é˜ˆå€¼
        self.hard_limit = hard_limit  # è§¦å‘ Pause çš„ç¡¬é˜ˆå€¼
        self.max_wait = max_wait

        # ç¼“å†²æ± 
        self.event_buffer: Set[Tuple[str, int]] = set()

        # é‡è¯•æ± : {car_id: retry_count}
        self.retry_buffer: Dict[int, int] = {}
        self.MAX_RETRIES = 3  # ğŸ”¥ æœ€å¤§é‡è¯•æ¬¡æ•°

        self.last_flush_time = time.time()
        self._lock = asyncio.Lock()
        self.paused = False

    async def add_event(self, table: str, row_id: int):
        """æ·»åŠ äº‹ä»¶å¹¶æ‰§è¡Œæµæ§"""
        async with self._lock:
            self.event_buffer.add((table, row_id))
            current_size = len(self.event_buffer) + len(self.retry_buffer)

        # 1. ğŸ›‘ æµæ§ (Backpressure): è¶…è¿‡ç¡¬é˜ˆå€¼ï¼Œæš‚åœæ¶ˆè´¹
        if current_size >= self.hard_limit and not self.paused:
            logger.warning(f"ğŸ›‘ Buffer çˆ†æ»¡ ({current_size})ï¼Œæš‚åœæ¶ˆè´¹ Kafka...")
            self.consumer.pause(*self.consumer.assignment())
            self.paused = True

        # 2. è§¦å‘ Flush
        if current_size >= self.max_events:
            await self.flush()

    async def add_retry_ids(self, ids: List[int]):
        """å¤„ç†å¤±è´¥é‡è¯• & æ­»ä¿¡é˜Ÿåˆ— (DLQ)"""
        async with self._lock:
            for i in ids:
                count = self.retry_buffer.get(i, 0) + 1
                if count <= self.MAX_RETRIES:
                    self.retry_buffer[i] = count
                else:
                    # ğŸ’€ æ­»ä¿¡å¤„ç† (DLQ)
                    logger.error(f"ğŸ’€ [DLQ] ID {i} é‡è¯• {count} æ¬¡ä»å¤±è´¥ï¼Œä¸¢å¼ƒï¼(è¯·äººå·¥ä»‹å…¥)")
                    # è¿™é‡Œå¯ä»¥å°† i å†™å…¥ DB çš„ dead_letter_queue è¡¨æˆ–å‘ç»™æŠ¥è­¦ç¾¤

    async def flush(self):
        """æ ¸å¿ƒåŒæ­¥é€»è¾‘"""
        async with self._lock:
            if not self.event_buffer and not self.retry_buffer:
                return

            # æå–å¿«ç…§
            events = list(self.event_buffer)
            # é‡è¯•çš„ ID ä¹Ÿå‚ä¸æœ¬æ¬¡è§£æå’ŒåŒæ­¥
            retry_ids = list(self.retry_buffer.keys())

            self.event_buffer.clear()
            # æ³¨æ„ï¼šretry_buffer ä¸æ¸…ç©ºï¼Œè€Œæ˜¯ç­‰å¤„ç†å®Œå¦‚æœæˆåŠŸäº†å†ç§»é™¤ï¼Œæˆ–è€…å¤±è´¥äº† update
            # ç®€åŒ–é€»è¾‘ï¼šå…ˆæ¸…ç©º retry_bufferï¼Œå¤±è´¥çš„å†åŠ å›æ¥ï¼ˆå¸¦ä¸Šç´¯åŠ çš„ countï¼‰
            # ä½†ä¸ºäº†ä¿æŒ countï¼Œè¿™é‡Œæš‚æ—¶å…¨éƒ¨æ¸…ç©ºï¼Œå¤„ç†å¤±è´¥æ—¶ add_retry_ids ä¼šå¤„ç† count
            # ä¿®æ­£ï¼šretry_buffer å­˜çš„æ˜¯ ID->Countã€‚
            # æˆ‘ä»¬æŠŠ ID æ‹¿å‡ºæ¥å¤„ç†ï¼Œå¦‚æœæˆåŠŸäº†å°±æ²¡äº‹äº†ã€‚å¦‚æœå¤±è´¥äº†ï¼Œadd_retry_ids ä¼šè¯»æ—§ count å—ï¼Ÿ
            # ä¸ä¼šï¼Œå› ä¸ºæˆ‘ä»¬æŠŠ retry_buffer clear äº†ã€‚
            # ğŸ’¡ ä¿®æ­£ç­–ç•¥ï¼šæš‚å­˜æ—§çš„ retry counts
            old_retry_counts = self.retry_buffer.copy()
            self.retry_buffer.clear()

            self.last_flush_time = time.time()

            # â–¶ï¸ æ¢å¤æ¶ˆè´¹ (å¦‚æœä¹‹å‰æš‚åœäº†)
            if self.paused:
                logger.info("â–¶ï¸ Buffer å‹åŠ›é‡Šæ”¾ï¼Œæ¢å¤æ¶ˆè´¹ Kafka...")
                self.consumer.resume(*self.consumer.assignment())
                self.paused = False

        # 1. è§£ææ‰€æœ‰å—å½±å“çš„ Car ID
        impacted_ids = set(old_retry_counts.keys())
        if events:
            resolved = await self._batch_resolve_events(events)
            impacted_ids.update(resolved)

        if not impacted_ids:
            await self._commit_offset()
            return

        logger.info(f"âš¡ [Flush] å¤„ç† {len(impacted_ids)} ä¸ª Car ID (å«é‡è¯• {len(old_retry_counts)})")

        # 2. æ‰¹é‡å¤„ç† (Batch Process)
        all_ids = list(impacted_ids)
        chunk_size = 500

        for i in range(0, len(all_ids), chunk_size):
            batch_ids = all_ids[i: i + chunk_size]

            # A. æŸ¥åº“ (Fetch)
            # æ•°æ®åº“é‡Œå­˜åœ¨çš„ -> Upsert
            # æ•°æ®åº“é‡Œä¸å­˜åœ¨çš„ -> Delete (è¿™å°±æ˜¯ Delete ç»Ÿä¸€å¤„ç†çš„æ ¸å¿ƒ)
            found_docs = await fetch_and_assemble_car_docs(batch_ids)
            found_ids = {d['id'] for d in found_docs}

            # è®¡ç®—éœ€è¦åˆ é™¤çš„ ID (è¯·æ±‚äº†ä½†æ²¡æŸ¥åˆ°ï¼Œè¯´æ˜è¢«åˆ äº†)
            missing_ids = [bid for bid in batch_ids if bid not in found_ids]

            # B. å†™å…¥ ES (Upsert)
            failed_upsert = []
            if found_docs:
                failed_upsert = await CarESService.bulk_sync_cars(found_docs)

            # C. å†™å…¥ ES (Delete)
            failed_delete = []
            if missing_ids:
                logger.info(f"ğŸ—‘ï¸ æ£€æµ‹åˆ° {len(missing_ids)} æ¡æ•°æ®å·²ä» DB åˆ é™¤ï¼ŒåŒæ­¥åˆ é™¤ ES...")
                failed_delete = await CarESService.bulk_delete_cars(missing_ids)

            # D. é”™è¯¯å¤„ç†ä¸é‡è¯•è®¡æ•°æ¢å¤
            current_failed = set(failed_upsert + failed_delete)
            if current_failed:
                # æ¢å¤ retry count
                ids_to_restore = []
                async with self._lock:
                    for fid in current_failed:
                        # å¦‚æœæ˜¯ä¹‹å‰å°±åœ¨ retry åˆ—è¡¨é‡Œçš„ï¼Œæ¢å¤è®¡æ•°ï¼›å¦‚æœæ˜¯æ–°çš„ï¼Œè®¡æ•°ä¸º 0
                        prev_count = old_retry_counts.get(fid, 0)
                        # æ‰‹åŠ¨å¡å›å»
                        self.retry_buffer[fid] = prev_count  # å…ˆæ¢å¤ï¼Œå†è°ƒç”¨ add å¢åŠ 

                await self.add_retry_ids(list(current_failed))

        # 3. æ‰‹åŠ¨æäº¤ Offset
        await self._commit_offset()

    async def _batch_resolve_events(self, events: List[Tuple[str, int]]) -> Set[int]:
        """æ‰¹é‡åæŸ¥ (å¸¦ SQL IN ä¸Šé™åˆ‡åˆ†)"""
        final_ids = set()

        # åˆ†ç»„
        model_ids = [eid for t, eid in events if t == 'car_model']
        series_ids = [eid for t, eid in events if t == 'car_series']
        brand_ids = [eid for t, eid in events if t == 'car_brand']

        final_ids.update(model_ids)

        async with AsyncSessionLocal() as session:
            # âœ‚ï¸ Chunking: series_ids åˆ†æ‰¹æŸ¥è¯¢
            chunk_size = 500
            for i in range(0, len(series_ids), chunk_size):
                chunk = series_ids[i:i + chunk_size]
                if not chunk: continue
                stmt = select(CarModel.id).where(CarModel.series_id.in_(chunk))
                res = await session.execute(stmt)
                final_ids.update(res.scalars().all())

            # âœ‚ï¸ Chunking: brand_ids åˆ†æ‰¹æŸ¥è¯¢
            for i in range(0, len(brand_ids), chunk_size):
                chunk = brand_ids[i:i + chunk_size]
                if not chunk: continue
                stmt = (
                    select(CarModel.id)
                    .join(CarSeries, CarModel.series_id == CarSeries.id)
                    .where(CarSeries.brand_id.in_(chunk))
                )
                res = await session.execute(stmt)
                final_ids.update(res.scalars().all())

        return final_ids

    async def _commit_offset(self):
        try:
            await self.consumer.commit()
        except Exception as e:
            logger.error(f"âŒ Offset Commit Failed: {e}")

    async def auto_flush_loop(self):
        while True:
            await asyncio.sleep(0.5)
            async with self._lock:
                should_flush = (
                        (self.event_buffer or self.retry_buffer) and
                        (time.time() - self.last_flush_time > self.max_wait)
                )
            if should_flush:
                await self.flush()


# ==========================================
# ğŸš€ å¯åŠ¨å…¥å£
# ==========================================
async def consume():
    await CarESService.create_index_if_not_exists()

    consumer = AIOKafkaConsumer(
        'cdc.car.car_model',
        'cdc.car.car_series',
        'cdc.car.car_brand',
        bootstrap_servers='localhost:9092',
        group_id='es_sync_group_max',  # Pro Max
        enable_auto_commit=False,  # ğŸ”¥ å¿…é¡»å…³é—­è‡ªåŠ¨æäº¤
        auto_offset_reset='latest'
    )

    await consumer.start()
    logger.info("ğŸš€ [Pro Max] CDC Consumer Started (Flow Control + DLQ + Unified Delete)")

    # è°ƒä¼˜å‚æ•°ï¼šHard Limit = 5000 è§¦å‘æš‚åœ
    buffer = SmartBuffer(consumer, max_events=2000, hard_limit=5000, max_wait=1.0)
    asyncio.create_task(buffer.auto_flush_loop())

    try:
        async for msg in consumer:
            if not msg.value: continue
            try:
                data = json.loads(msg.value)
                payload = data.get('payload')
                if not payload: continue

                op = payload['op']
                row = payload.get('before') if op == 'd' else payload.get('after')
                if not row: continue

                table = msg.topic.split('.')[-1]
                row_id = row['id']

                # ğŸ”¥ æ— è®ºå¢åˆ æ”¹ï¼Œç»Ÿç»Ÿå…¥é˜Ÿï¼ŒFlush æ—¶å†å»æŸ¥åº“å®šå¤º
                # è¿™æ ·å®Œç¾è§£å†³äº† Delete å’Œ Update ä¹±åºçš„é—®é¢˜
                await buffer.add_event(table, row_id)

            except Exception as e:
                logger.error(f"âŒ Parse Error: {e}")
    finally:
        await consumer.stop()


if __name__ == "__main__":
    try:
        asyncio.run(consume())
    except KeyboardInterrupt:
        pass