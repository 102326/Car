# app/utils/model_factory.py
"""
å¤šæ¨¡å‹å·¥å‚ï¼šæ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©åˆé€‚çš„æ¨¡å‹
"""
import os
from enum import Enum
from typing import Optional
from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI
from langchain_core.callbacks import StdOutCallbackHandler
from dotenv import load_dotenv
from app.config import settings

load_dotenv(override=True)


class ModelType(str, Enum):
    """æ¨¡å‹ç±»å‹æšä¸¾"""
    BRAIN = "brain"        # å¤§è„‘ï¼šå¤æ‚æ¨ç†ã€ä»£ç ç”Ÿæˆ
    QUICK = "quick"        # å¿«å˜´ï¼šé—²èŠã€ç®€å•ä»»åŠ¡
    VISION = "vision"      # çœ¼ç›ï¼šè§†è§‰è¯†åˆ«
    EMBEDDING = "embedding"  # å›¾ä¹¦é¦†ç®¡ç†å‘˜ï¼šå‘é‡åŒ–


class ModelFactory:
    """
    å¤šæ¨¡å‹å·¥å‚
    
    æ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„æ¨¡å‹ï¼š
    - brain (å¤§è„‘): qwen2.5-coder:14b - å¤æ‚æ¨ç†ã€æ„å›¾è¯†åˆ«ã€æ•°æ®æå–
    - quick (å¿«å˜´): qwen2.5:7b - é—²èŠã€ç®€å•é—®ç­”
    - vision (çœ¼ç›): minicpm-v - å›¾åƒè¯†åˆ«ï¼ˆé¢„ç•™ï¼‰
    - embedding (å›¾ä¹¦é¦†ç®¡ç†å‘˜): bge-m3 - æ–‡æœ¬å‘é‡åŒ–
    """
    
    # æ¨¡å‹é…ç½®
    MODELS = {
        ModelType.BRAIN: {
            "name": "qwen2.5-coder:14b",
            "description": "å¤§è„‘ - å¤æ‚æ¨ç†å’Œä»£ç ç”Ÿæˆ",
            "temperature": 0.3,  # è¾ƒä½æ¸©åº¦ï¼Œä¿è¯ç¨³å®šæ€§
            "use_cases": ["æ„å›¾è¯†åˆ«", "æ•°æ®æå–", "æŸ¥è¯¢è§£æ", "å¤æ‚æ¨ç†"]
        },
        ModelType.QUICK: {
            "name": "qwen2.5:7b",
            "description": "å¿«å˜´ - é—²èŠå’Œç®€å•ä»»åŠ¡",
            "temperature": 0.8,  # è¾ƒé«˜æ¸©åº¦ï¼Œæ›´è‡ªç„¶
            "use_cases": ["é—²èŠ", "æ‰“æ‹›å‘¼", "ç®€å•é—®ç­”", "å¿«é€Ÿå“åº”"]
        },
        ModelType.VISION: {
            "name": "minicpm-v",
            "description": "çœ¼ç› - è§†è§‰è¯†åˆ«",
            "temperature": 0.5,
            "use_cases": ["å›¾åƒè¯†åˆ«", "OCR", "è½¦è¾†è¯†åˆ«"]
        }
    }
    
    @staticmethod
    def get_callbacks():
        """è·å–å›è°ƒå¤„ç†å™¨"""
        return [StdOutCallbackHandler()]
    
    @staticmethod
    def get_llm(
        model_type: ModelType = ModelType.BRAIN,
        temperature: Optional[float] = None,
        verbose: bool = False
    ) -> ChatOllama:
        """
        æ ¹æ®æ¨¡å‹ç±»å‹è·å– LLM
        
        Args:
            model_type: æ¨¡å‹ç±»å‹ï¼ˆbrain/quick/visionï¼‰
            temperature: æ¸©åº¦ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨é¢„è®¾å€¼ï¼‰
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
        
        Returns:
            ChatOllama å®ä¾‹
        
        ä½¿ç”¨ç¤ºä¾‹:
            # å¤æ‚æ¨ç†ï¼ˆæ„å›¾è¯†åˆ«ã€æ•°æ®æå–ï¼‰
            llm = ModelFactory.get_llm(ModelType.BRAIN)
            
            # ç®€å•é—²èŠ
            llm = ModelFactory.get_llm(ModelType.QUICK)
        """
        # ä¼˜å…ˆä½¿ç”¨ DeepSeek APIï¼ˆå¦‚æœé…ç½®äº†ï¼‰
        api_key = os.getenv("DEEPSEEK_API_KEY") or getattr(settings, "DEEPSEEK_API_KEY", "")
        
        if api_key:
            print(f"âš¡ [Model] ä½¿ç”¨ DeepSeek API (deepseek-chat)")
            return ChatOpenAI(
                model="deepseek-chat",
                openai_api_key=api_key,
                openai_api_base="https://api.deepseek.com",
                temperature=temperature or 0.7,
                callbacks=ModelFactory.get_callbacks() if verbose else None,
                verbose=verbose
            )
        
        # ä½¿ç”¨æœ¬åœ° Ollama æ¨¡å‹
        model_config = ModelFactory.MODELS.get(model_type)
        if not model_config:
            raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹: {model_type}")
        
        model_name = model_config["name"]
        default_temp = model_config["temperature"]
        
        print(f"ğŸ¤– [Model] ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {model_name} ({model_config['description']})")
        
        return ChatOllama(
            model=model_name,
            base_url="http://localhost:11434",
            temperature=temperature if temperature is not None else default_temp,
            callbacks=ModelFactory.get_callbacks() if verbose else None
        )
    
    @staticmethod
    def get_brain_model(temperature: float = 0.3) -> ChatOllama:
        """
        è·å–"å¤§è„‘"æ¨¡å‹ï¼ˆqwen2.5-coder:14bï¼‰
        
        é€‚ç”¨åœºæ™¯ï¼š
        - æ„å›¾è¯†åˆ«ï¼ˆIntent Routerï¼‰
        - æŸ¥è¯¢è§£æï¼ˆQuery Parserï¼‰
        - æ•°æ®æå–ï¼ˆData Extractionï¼‰
        - å¤æ‚æ¨ç†
        """
        return ModelFactory.get_llm(ModelType.BRAIN, temperature)
    
    @staticmethod
    def get_quick_model(temperature: float = 0.8) -> ChatOllama:
        """
        è·å–"å¿«å˜´"æ¨¡å‹ï¼ˆqwen2.5:7bï¼‰
        
        é€‚ç”¨åœºæ™¯ï¼š
        - é—²èŠï¼ˆChat Nodeï¼‰
        - æ‰“æ‹›å‘¼
        - ç®€å•é—®ç­”
        - RAG ç”Ÿæˆï¼ˆåŸºäºæ£€ç´¢ç»“æœç”Ÿæˆç­”æ¡ˆï¼‰
        """
        return ModelFactory.get_llm(ModelType.QUICK, temperature)


# ==========================================
# å‘é‡åŒ–æ¨¡å‹ï¼ˆBGE-M3ï¼‰
# ==========================================
class EmbeddingModel:
    """
    å‘é‡åŒ–æ¨¡å‹ï¼šbge-m3
    
    ä½¿ç”¨ sentence-transformers åŠ è½½æœ¬åœ°æ¨¡å‹
    """
    
    _instance = None
    _model = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if self._model is None:
            self._load_model()
    
    def _load_model(self):
        """
        åŠ è½½ BGE-M3 æ¨¡å‹
        
        ä¼˜å…ˆåŠ è½½æœ¬åœ°æ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™è‡ªåŠ¨ä¸‹è½½
        """
        try:
            from sentence_transformers import SentenceTransformer
            import os
            from pathlib import Path
            
            print("ğŸ“š [Embedding] åŠ è½½ bge-m3 æ¨¡å‹...")
            
            # æ£€æŸ¥æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
            model_paths = [
                Path("D:/biancheng/models/bge-m3"),  # ä¼˜å…ˆï¼šç”¨æˆ·æŒ‡å®šçš„ç»Ÿä¸€æ¨¡å‹ç›®å½•
                Path(__file__).parent.parent.parent / "models" / "bge-m3"  # å¤‡ç”¨ï¼šé¡¹ç›®ç›®å½•
            ]
            
            local_model_path = None
            for path in model_paths:
                if path.exists():
                    local_model_path = path
                    break
            
            if local_model_path:
                # ä½¿ç”¨æœ¬åœ°æ¨¡å‹
                print(f"   ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {local_model_path}")
                self._model = SentenceTransformer(
                    str(local_model_path),
                    trust_remote_code=True
                )
            else:
                # è‡ªåŠ¨ä¸‹è½½ï¼ˆä½¿ç”¨å›½å†…é•œåƒï¼‰
                print("   æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œä»é•œåƒæºä¸‹è½½...")
                os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
                
                self._model = SentenceTransformer(
                    "BAAI/bge-m3",
                    trust_remote_code=True,
                    cache_folder=None
                )
            
            print(f"âœ… [Embedding] bge-m3 æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âš ï¸ [Embedding] bge-m3 æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("\næç¤ºï¼š")
            print("  è¯·å°†æ¨¡å‹ä¸‹è½½åˆ°ä»¥ä¸‹ä»»ä¸€ä½ç½®ï¼š")
            print("    1. D:\\biancheng\\models\\bge-m3 ï¼ˆæ¨èï¼‰")
            print("    2. <é¡¹ç›®ç›®å½•>\\models\\bge-m3")
            print("\n  ä¸‹è½½å‘½ä»¤ï¼š")
            print("    modelscope download --model ZhipuAI/bge-m3 --local_dir D:\\biancheng\\models\\bge-m3")
            self._model = None
    
    def encode(self, texts, batch_size: int = 32, show_progress: bool = False):
        """
        å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
        
        Args:
            texts: å•ä¸ªæ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
        
        Returns:
            numpy.ndarray: å‘é‡ï¼ˆå•ä¸ªæ–‡æœ¬ï¼‰æˆ–å‘é‡åˆ—è¡¨ï¼ˆå¤šä¸ªæ–‡æœ¬ï¼‰
        """
        if self._model is None:
            raise RuntimeError("Embedding æ¨¡å‹æœªåŠ è½½")
        
        return self._model.encode(
            texts,
            batch_size=batch_size,
            show_progress_bar=show_progress,
            normalize_embeddings=True  # å½’ä¸€åŒ–ï¼Œä¾¿äºè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        )
    
    @property
    def dimension(self) -> int:
        """å‘é‡ç»´åº¦"""
        if self._model is None:
            return 0
        return self._model.get_sentence_embedding_dimension()


# ==========================================
# é‡æ’åºæ¨¡å‹ï¼ˆBGE Rerankerï¼‰
# ==========================================
class RerankerModel:
    """
    é‡æ’åºæ¨¡å‹ï¼šbge-reranker-v2-m3
    
    ç”¨äºå¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’åºï¼Œæå‡ç›¸å…³æ€§
    """
    
    _instance = None
    _model = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if self._model is None:
            self._load_model()
    
    def _load_model(self):
        """
        åŠ è½½ BGE Reranker æ¨¡å‹
        
        ä¼˜å…ˆåŠ è½½æœ¬åœ°æ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™è‡ªåŠ¨ä¸‹è½½
        """
        try:
            from sentence_transformers import CrossEncoder
            import os
            from pathlib import Path
            
            print("ğŸ”„ [Reranker] åŠ è½½ bge-reranker-v2-m3 æ¨¡å‹...")
            
            # æ£€æŸ¥æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
            model_paths = [
                Path("D:/biancheng/models/bge-reranker-v2-m3"),  # ä¼˜å…ˆï¼šç”¨æˆ·æŒ‡å®šçš„ç»Ÿä¸€æ¨¡å‹ç›®å½•
                Path(__file__).parent.parent.parent / "models" / "bge-reranker-v2-m3"  # å¤‡ç”¨ï¼šé¡¹ç›®ç›®å½•
            ]
            
            local_model_path = None
            for path in model_paths:
                if path.exists():
                    local_model_path = path
                    break
            
            if local_model_path:
                # ä½¿ç”¨æœ¬åœ°æ¨¡å‹
                print(f"   ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {local_model_path}")
                self._model = CrossEncoder(
                    str(local_model_path),
                    max_length=512,
                    trust_remote_code=True
                )
            else:
                # è‡ªåŠ¨ä¸‹è½½ï¼ˆä½¿ç”¨å›½å†…é•œåƒï¼‰
                print("   æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œä»é•œåƒæºä¸‹è½½...")
                os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
                
                self._model = CrossEncoder(
                    "BAAI/bge-reranker-v2-m3",
                    max_length=512,
                    trust_remote_code=True
                )
            
            print(f"âœ… [Reranker] bge-reranker-v2-m3 æ¨¡å‹åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âš ï¸ [Reranker] bge-reranker-v2-m3 æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("\næç¤ºï¼š")
            print("  è¯·å°†æ¨¡å‹ä¸‹è½½åˆ°ä»¥ä¸‹ä»»ä¸€ä½ç½®ï¼š")
            print("    1. D:\\biancheng\\models\\bge-reranker-v2-m3 ï¼ˆæ¨èï¼‰")
            print("    2. <é¡¹ç›®ç›®å½•>\\models\\bge-reranker-v2-m3")
            print("\n  ä¸‹è½½å‘½ä»¤ï¼š")
            print("    modelscope download --model ZhipuAI/bge-reranker-v2-m3 --local_dir D:\\biancheng\\models\\bge-reranker-v2-m3")
            self._model = None
    
    def rerank(self, query: str, documents: list, top_k: int = 10):
        """
        é‡æ’åºæ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: æ–‡æ¡£åˆ—è¡¨ï¼ˆå­—ç¬¦ä¸²æˆ–å­—å…¸ï¼Œå¦‚æœæ˜¯å­—å…¸éœ€è¦æœ‰ 'text' æˆ– 'content' å­—æ®µï¼‰
            top_k: è¿”å›å‰ k ä¸ªç»“æœ
        
        Returns:
            æ’åºåçš„æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æ¡£åŒ…å« 'score' å­—æ®µ
        """
        if self._model is None:
            raise RuntimeError("Reranker æ¨¡å‹æœªåŠ è½½")
        
        # æå–æ–‡æœ¬
        texts = []
        for doc in documents:
            if isinstance(doc, str):
                texts.append(doc)
            elif isinstance(doc, dict):
                texts.append(doc.get('text') or doc.get('content') or doc.get('name', ''))
            else:
                texts.append(str(doc))
        
        # æ„é€  query-document å¯¹
        pairs = [[query, text] for text in texts]
        
        # è®¡ç®—ç›¸å…³æ€§å¾—åˆ†
        scores = self._model.predict(pairs)
        
        # æ’åº
        sorted_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)
        
        # æ„é€ è¿”å›ç»“æœ
        reranked = []
        for idx in sorted_indices[:top_k]:
            doc = documents[idx]
            if isinstance(doc, dict):
                doc['rerank_score'] = float(scores[idx])
            else:
                doc = {'text': doc, 'rerank_score': float(scores[idx])}
            reranked.append(doc)
        
        return reranked


# ==========================================
# å•ä¾‹å®ä¾‹
# ==========================================
embedding_model = EmbeddingModel()
reranker_model = RerankerModel()
