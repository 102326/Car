from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.interval import IntervalTrigger
from app.services.news_crawler import AutoNewsCrawler
from app.models.Content_Resource import CMSPost, PostType
from app.database import AsyncSessionLocal
from sqlalchemy import select
import logging

logger = logging.getLogger("uvicorn")
scheduler = AsyncIOScheduler()


async def scheduled_crawl_task():
    """
    å®šæ—¶çˆ¬è™«ä»»åŠ¡é€»è¾‘
    """
    logger.info("ğŸ•·ï¸ [å®šæ—¶ä»»åŠ¡] å¼€å§‹æ‰§è¡Œå…¨ç½‘èµ„è®¯æŠ“å–...")
    crawler = AutoNewsCrawler()
    try:
        crawl_result = await crawler.run_all()
        logger.info(f"ğŸ“Š æŠ“å–ç»Ÿè®¡: æ‡‚è½¦å¸[{len(crawl_result['dongchedi'])}] æ±½è½¦ä¹‹å®¶[{len(crawl_result['autohome'])}] æ˜“è½¦[{len(crawl_result['yiche'])}]")
        all_articles = crawl_result["all_flat"]

        if not all_articles:
            logger.info("âš ï¸ [å®šæ—¶ä»»åŠ¡] æœ¬æ¬¡æœªæŠ“å–åˆ°æ•°æ®")
            return

        new_count = 0
        admin_user_id = 1

        # å¿…é¡»æ‰‹åŠ¨ç®¡ç† Sessionï¼Œå› ä¸ºä¸åœ¨ Request ä¸Šä¸‹æ–‡ä¸­
        async with AsyncSessionLocal() as db:
            for item in all_articles:
                try:
                    # å»é‡
                    stmt = select(CMSPost).where(CMSPost.content_body == item["url"])
                    result = await db.execute(stmt)
                    if result.scalars().first():
                        continue

                    new_post = CMSPost(
                        user_id=admin_user_id,
                        title=item["title"],
                        post_type=PostType.ARTICLE,
                        cover_url=item["cover"],
                        content_body=item["url"],  # å­˜å…¥ç°æœ‰å­—æ®µ
                        status=1,
                        ip_location=f"è‡ªåŠ¨çˆ¬å–|{item['source']}"
                    )
                    db.add(new_post)
                    new_count += 1
                except Exception as e:
                    logger.error(f"âŒ å…¥åº“å¤±è´¥: {e}")

            await db.commit()
            logger.info(f"âœ… [å®šæ—¶ä»»åŠ¡] æŠ“å–å®Œæˆï¼Œæ–°å¢æ–‡ç« : {new_count} ç¯‡")

    except Exception as e:
        logger.error(f"âŒ [å®šæ—¶ä»»åŠ¡] çˆ¬è™«è¿è¡Œå¼‚å¸¸: {e}")


def start_scheduler():
    """
    å¯åŠ¨è°ƒåº¦å™¨å¹¶æ·»åŠ ä»»åŠ¡
    """
    # æ¯éš” 1 å°æ—¶æŠ“å–ä¸€æ¬¡
    scheduler.add_job(
        scheduled_crawl_task,
        # æ¯5ç§’æ‰§è¡Œä¸€æ¬¡
        trigger=IntervalTrigger(hours=12),
        id="news_crawler",
        replace_existing=True,
        max_instances=1,
        coalesce=True 
    )
    scheduler.start()
    logger.info("â° [ç³»ç»Ÿ] å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²å¯åŠ¨ (å‘¨æœŸ:ä¸€ä¸ªå°æ—¶)")