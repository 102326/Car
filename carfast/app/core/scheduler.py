from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.interval import IntervalTrigger
from app.services.news_crawler import AutoNewsCrawler
from app.models.Content_Resource import CMSPost, PostType
from app.core.database import AsyncSessionLocal
from app.models.user import UserAuth
from sqlalchemy import select
import logging

logger = logging.getLogger("uvicorn")
scheduler = AsyncIOScheduler()


async def scheduled_crawl_task():
    """
    å®šæ—¶çˆ¬è™«ä»»åŠ¡é€»è¾‘ (å¢å¼ºå¥å£®æ€§ç‰ˆ)
    åŒæ­¥äº† admin_tool.py ä¸­çš„å­—æ®µæˆªæ–­å’Œé€æ¡æäº¤é€»è¾‘
    """
    logger.info("ğŸ•·ï¸ [å®šæ—¶ä»»åŠ¡] å¼€å§‹æ‰§è¡Œå…¨ç½‘èµ„è®¯æŠ“å–...")
    crawler = AutoNewsCrawler()
    try:
        # 1. çˆ¬å–
        crawl_result = await crawler.run_all()
        all_articles = crawl_result["all_flat"]

        if not all_articles:
            logger.info("âš ï¸ [å®šæ—¶ä»»åŠ¡] æœ¬æ¬¡æœªæŠ“å–åˆ°æ•°æ®")
            return

        # 2. å…¥åº“
        async with AsyncSessionLocal() as db:

            # --- ç”¨æˆ·è‡ªåŠ¨ä¿®æ­£é€»è¾‘ ---
            # å¿…é¡»ç¡®ä¿æœ‰ç®¡ç†å‘˜ç”¨æˆ·ï¼Œå¦åˆ™å…¥åº“ä¼šæŠ¥å¤–é”®é”™è¯¯
            admin_user = await db.get(UserAuth, 1)
            if not admin_user:
                logger.warning("âš ï¸ [å®šæ—¶ä»»åŠ¡] ç®¡ç†å‘˜(ID=1)ä¸å­˜åœ¨ï¼Œå°è¯•åˆ›å»º...")
                try:
                    admin_user = UserAuth(id=1, phone="13800000000", status=1)
                    db.add(admin_user)
                    await db.flush()
                except Exception:
                    await db.rollback()
                    # å°è¯•è·å–ä»»æ„ä¸€ä¸ªç”¨æˆ·ä½œä¸ºå…œåº•
                    stmt = select(UserAuth).limit(1)
                    res = await db.execute(stmt)
                    admin_user = res.scalars().first()

            if not admin_user:
                logger.error("âŒ [å®šæ—¶ä»»åŠ¡] ä¸¥é‡é”™è¯¯ï¼šæ•°æ®åº“æ— ä»»ä½•ç”¨æˆ·ï¼Œæ— æ³•å½’æ¡£æ–‡ç« ï¼")
                return

            admin_user_id = admin_user.id
            new_count = 0

            for item in all_articles:
                try:
                    # å»é‡æ£€æŸ¥
                    stmt = select(CMSPost).where(CMSPost.content_body == item["url"])
                    result = await db.execute(stmt)
                    if result.scalars().first():
                        continue

                    # === å…³é”®ä¿®å¤ï¼šå­—æ®µå®‰å…¨æˆªæ–­ ===
                    safe_title = item["title"][:99] if item["title"] else "æ— æ ‡é¢˜"
                    safe_cover = item["cover"][:254] if item["cover"] else ""

                    new_post = CMSPost(
                        user_id=admin_user_id,
                        title=safe_title,
                        post_type=PostType.ARTICLE,
                        cover_url=safe_cover,
                        content_body=item["url"],
                        status=1,
                        ip_location=f"è‡ªåŠ¨çˆ¬å–|{item['source']}"
                    )
                    db.add(new_post)

                    # === å…³é”®ä¿®å¤ï¼šé€æ¡æäº¤ ===
                    # é¿å…ä¸€æ¡å¤±è´¥å¯¼è‡´æ•´æ‰¹å›æ»š
                    await db.commit()
                    new_count += 1

                except Exception as e:
                    await db.rollback()
                    logger.error(f"âŒ å•æ¡å…¥åº“å¤±è´¥: {e} | æ ‡é¢˜: {item.get('title', '')}")

            logger.info(f"âœ… [å®šæ—¶ä»»åŠ¡] æŠ“å–å®Œæˆï¼ŒæˆåŠŸå…¥åº“: {new_count} ç¯‡")

    except Exception as e:
        logger.error(f"âŒ [å®šæ—¶ä»»åŠ¡] çˆ¬è™«è¿è¡Œå¼‚å¸¸: {e}")


def start_scheduler():
    """
    å¯åŠ¨è°ƒåº¦å™¨å¹¶æ·»åŠ ä»»åŠ¡
    """
    # æ¯éš” 1 å°æ—¶æŠ“å–ä¸€æ¬¡
    scheduler.add_job(
        scheduled_crawl_task,
        trigger=IntervalTrigger(hours=1),
        id="news_crawler",
        replace_existing=True,
        max_instances=1,
        coalesce=True
    )
    scheduler.start()
    logger.info("â° [ç³»ç»Ÿ] å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²å¯åŠ¨ (å‘¨æœŸ:ä¸€ä¸ªå°æ—¶)")
